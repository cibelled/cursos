apiVersion: "kafka.strimzi.io/v1beta2"
kind: "KafkaConnector"
metadata:
  # connector name
  name: "ingest-src-sqlserver-cdc-tables-avro-2bbc5953"
  labels:
    # kafka connect [cluster] name
    strimzi.io/cluster: edh
spec:
  class: io.debezium.connector.sqlserver.SqlServerConnector
  tasksMax: 1
  config:
    key.converter: "io.confluent.connect.avro.AvroConverter"
    key.converter.schema.registry.url: "http://schema-registry-cp-schema-registry:8081"
    value.converter: "io.confluent.connect.avro.AvroConverter"
    value.converter.schema.registry.url: "http://schema-registry-cp-schema-registry:8081"
    database.hostname: "20.75.27.11"
    database.port: "1433"
    database.dbname: "OwsHQ"
    database.user: "${file:/opt/kafka/external-configuration/connector-config-mssql/mssql-credentials.properties:mssql_username}"
    database.password: "${file:/opt/kafka/external-configuration/connector-config-mssql/mssql-credentials.properties:mssql_password}"
    table.whitelist: "dbo.credit_card"
    database.server.name: "owshq.sqlserver"
    database.history.kafka.bootstrap.servers: "edh-kafka-bootstrap:9092"
    database.history.kafka.topic: "ddl-dbhistory-cdc-tables"
    decimal.handling.mode: "double"
    tombstones.onâ€‹.delete: "true"
    poll.interval.ms: 20000
    max.batch.size: 600
    max.queue.size: 1200
    query.fetch.size: 1000
    snapshot.fetch.size: 600
    transforms: "unwrap"
    transforms.unwrap.type: "io.debezium.transforms.ExtractNewRecordState"
    transforms.unwrap.add.headers: "db"
    transforms.unwrap.add.fields: "op,table,source.ts_ms"
    transforms.unwrap.drop.tombstones: "false"
    transforms.unwrap.delete.handling.mode: "rewrite"
